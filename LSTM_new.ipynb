{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import Masking\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl NaN-Werte pro Spalte:\n",
      " Date time              0\n",
      "Location               0\n",
      "shortwave_radiation    0\n",
      "Temperature            0\n",
      "AOI                    0\n",
      "Cloud Cover            0\n",
      "Relative Humidity      0\n",
      "kW/kWp                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(r'C:\\Users\\opper\\Master-Arbeit\\data\\preprocessed\\pv_forecast.db')\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \"Date time\", Location, shortwave_radiation, Temperature, AOI, \"Cloud Cover\", \"Relative Humidity\", \"kW/kWp\"\n",
    "FROM pv_weather_data\n",
    "ORDER BY Location, \"Date time\" ASC\n",
    "\"\"\"\n",
    "# Daten abrufen und in ein DataFrame laden\n",
    "df = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Ausgabe des DataFrames zum Überprüfen\n",
    "\n",
    "\n",
    "print(\"Anzahl NaN-Werte pro Spalte:\\n\", df.isna().sum())\n",
    "\n",
    "# Falls es NaN-Werte gibt, betroffene Zeilen anzeigen\n",
    "if df.isna().sum().sum() > 0:\n",
    "    print(\"\\nZeilen mit NaN-Werten:\")\n",
    "    print(df[df.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sliding_window(df, datetime_col, feature_cols, target_col, window_size=480, forecast_horizon=96, step_size=1):\n",
    "    df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "    df = df.sort_values(by=datetime_col).reset_index(drop=True)\n",
    "\n",
    "    feature_data = df[feature_cols].values\n",
    "    target_data = df[target_col].values\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(0, len(df) - window_size - forecast_horizon, step_size):\n",
    "        X_seq = feature_data[i : i + window_size].copy()\n",
    "\n",
    "        X_seq[-forecast_horizon:, -1] = -1\n",
    "\n",
    "        y_seq = target_data[i + window_size : i + window_size + forecast_horizon]\n",
    "\n",
    "        X.append(X_seq)  # Input-Sequenz\n",
    "        y.append(y_seq)  # Zielwerte\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = [\"Date time\", \"Location\", \"shortwave_radiation\", \"Temperature\", \"AOI\", \"Cloud Cover\", \"Relative Humidity\", \"kW/kWp\"]\n",
    "missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Fehlende Spalten in der Datenbank: {missing_cols}\")\n",
    "\n",
    "df = df.sort_values(by=[\"Location\", \"Date time\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstelle Sliding Windows für bielefeldmel...\n",
      "Erstelle Sliding Windows für bielefeldref...\n",
      "Erstelle Sliding Windows für gaithersburg...\n",
      "Erstelle Sliding Windows für hongkong...\n",
      "Erstelle Sliding Windows für istanbul...\n",
      "Erstelle Sliding Windows für victoria14...\n",
      "Erstelle Sliding Windows für victoria15...\n",
      "Erstelle Sliding Windows für victoria16...\n",
      "Erstelle Sliding Windows für victoria17...\n",
      "Finaler Tensor X Shape: (438007, 480, 6)\n",
      "Finaler Tensor y Shape: (438007, 96)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\"shortwave_radiation\", \"Temperature\", \"AOI\", \"Cloud Cover\", \"Relative Humidity\", \"kW/kWp\"]\n",
    "target_col = \"kW/kWp\"\n",
    "\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "df[feature_cols] = feature_scaler.fit_transform(df[feature_cols])\n",
    "df[target_col] = target_scaler.fit_transform(df[[target_col]])\n",
    "\n",
    "\n",
    "X_list, y_list = [], []\n",
    "location_ids = []\n",
    "\n",
    "\n",
    "for location in df[\"Location\"].unique():\n",
    "    print(f\"Erstelle Sliding Windows für {location}...\")\n",
    "\n",
    "    df_location = df[df[\"Location\"] == location].copy()\n",
    "\n",
    "    X, y = get_sliding_window(df_location, datetime_col=\"Date time\", feature_cols=feature_cols, target_col=target_col)\n",
    "\n",
    "    X_list.append(X)\n",
    "    y_list.append(y)\n",
    "    \n",
    "\n",
    "    location_ids.extend([location] * len(X))\n",
    "\n",
    "X_final = np.concatenate(X_list, axis=0)  # Alle Input-Sequenzen\n",
    "y_final = np.concatenate(y_list, axis=0)  # Alle Zielwerte\n",
    "\n",
    "location_ids = np.array(location_ids)\n",
    "\n",
    "np.save(\"X_tensor.npy\", X_final)\n",
    "np.save(\"y_tensor.npy\", y_final)\n",
    "np.save(\"location_ids.npy\", location_ids)\n",
    "\n",
    "print(f\"Finaler Tensor X Shape: {X_final.shape}\")  # (Samples, Timesteps=672, Features=3)\n",
    "print(f\"Finaler Tensor y Shape: {y_final.shape}\")  # (Samples, Forecast_Horizon=96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finaler Tensor y Shape: (438007,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Finaler Tensor y Shape: {location_ids.shape}\")  # (Samples, Forecast_Horizon=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding Windows für istanbul: (37417, 480, 3)\n"
     ]
    }
   ],
   "source": [
    "# Lade die gespeicherten Tensoren\n",
    "X_final = np.load(\"X_tensor.npy\")\n",
    "y_final = np.load(\"y_tensor.npy\")\n",
    "location_ids = np.load(\"location_ids.npy\")  # Lade die Standort-IDs\n",
    "\n",
    "# Wähle einen Standort aus, z. B. \"Bielefeld\"\n",
    "location = \"istanbul\"\n",
    "\n",
    "# Finde die Indizes, an denen dieser Standort vorkommt\n",
    "indices = np.where(location_ids == location)[0]\n",
    "\n",
    "# Filtere die Sliding Windows für diesen Standort\n",
    "X_location = X_final[indices]\n",
    "y_location = y_final[indices]\n",
    "\n",
    "print(f\"Sliding Windows für {location}: {X_location.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstes Sliding Window für istanbul:\n",
      "[[ 0.          0.4914611   0.        ]\n",
      " [ 0.          0.48766603  0.        ]\n",
      " [ 0.          0.48766603  0.        ]\n",
      " ...\n",
      " [ 0.          0.5256167  -1.        ]\n",
      " [ 0.          0.5256167  -1.        ]\n",
      " [ 0.          0.5256167  -1.        ]]\n",
      "\n",
      "Erster Zeitschritt im Sliding Window für istanbul:\n",
      "[0.        0.4914611 0.       ]\n",
      "\n",
      "Zielwerte (y) für das erste Sliding Window für istanbul:\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.03295025 0.03295025\n",
      " 0.03295025 0.03295025 0.06689898 0.06689898 0.         0.13279947\n",
      " 0.06689898 0.06689898 0.03295025 0.19969846 0.03295025 0.19969846\n",
      " 0.13279947 0.09984923 0.13279947 0.16674821 0.06689898 0.13279947\n",
      " 0.06689898 0.13279947 0.16674821 0.16674821 0.         0.06689898\n",
      " 0.19969846 0.29954768 0.         0.29954768 0.33249793 0.19969846\n",
      " 0.33249793 0.2326487  0.2326487  0.19969846 0.16674821 0.03295025\n",
      " 0.09984923 0.         0.13279947 0.16674821 0.13279947 0.13279947\n",
      " 0.09984923 0.03295025 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "window_index = 0  \n",
    "\n",
    "print(f\"Erstes Sliding Window für {location}:\")\n",
    "print(X_location[window_index])\n",
    "\n",
    "print(f\"\\nErster Zeitschritt im Sliding Window für {location}:\")\n",
    "print(X_location[window_index, 0])  \n",
    "\n",
    "print(f\"\\nZielwerte (y) für das erste Sliding Window für {location}:\")\n",
    "print(y_location[window_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-25 19:36:01,465] A new study created in memory with name: no-name-1fb9a94b-c4b0-4930-bf7b-e1589eb9d73f\n",
      "C:\\Users\\opper\\AppData\\Local\\Temp\\ipykernel_4268\\4232760410.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    }
   ],
   "source": [
    "def hyperparameter_tuning(trial):\n",
    "    lstm_units_1 = trial.suggest_int('lstm_units_1', 16, 64, step=4)\n",
    "    lstm_units_2 = trial.suggest_int('lstm_units_2', 16, 128, step=8)\n",
    "    lstm_units_3 = trial.suggest_int('lstm_units_3', 32, 256, step=8)\n",
    "    dropout_1 = trial.suggest_float('dropout_1', 0.1, 0.5)\n",
    "    dropout_2 = trial.suggest_float('dropout_2', 0.1, 0.5)\n",
    "    dropout_3 = trial.suggest_float('dropout_3', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n",
    "\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test, loc_temp, loc_test = train_test_split(\n",
    "    X_final, y_final, location_ids,\n",
    "    test_size=0.15,\n",
    "    stratify=location_ids,\n",
    "    random_state=42)\n",
    "\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    val_losses = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_temp, y_temp, groups=loc_temp)):\n",
    "        X_train, X_val = X_temp[train_idx], X_temp[val_idx]\n",
    "        y_train, y_val = y_temp[train_idx], y_temp[val_idx]\n",
    "            \n",
    "        model = Sequential([\n",
    "            Masking(mask_value=-1, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            LSTM(lstm_units_1, activation=\"tanh\", return_sequences=True),\n",
    "            Dropout(dropout_1),\n",
    "            LSTM(lstm_units_2, activation=\"tanh\", return_sequences=True),\n",
    "            Dropout(dropout_2),\n",
    "            LSTM(lstm_units_3, activation=\"tanh\"),\n",
    "            Dropout(dropout_3), \n",
    "            Dense(96, activation='sigmoid')  # 96 Zeitschritte als Vorhersage (24h PV-Leistung)\n",
    "        ])\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=\"mse\", metrics=['mae'])\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=30,  \n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1  \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        val_losses.append(val_loss)\n",
    "    return np.mean(val_losses)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(hyperparameter_tuning, n_trials=20)\n",
    "\n",
    "print(\"Beste Hyperparameter:\", study.best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mstudy\u001b[49m\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m      3\u001b[0m final_model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      4\u001b[0m     Masking(mask_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(X_temp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_temp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])),\n\u001b[0;32m      5\u001b[0m     LSTM(best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_units_1\u001b[39m\u001b[38;5;124m'\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     Dense(\u001b[38;5;241m96\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m     12\u001b[0m ])\n\u001b[0;32m     14\u001b[0m final_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "\n",
    "final_model = Sequential([\n",
    "    Masking(mask_value=-1, input_shape=(X_temp.shape[1], X_temp.shape[2])),\n",
    "    LSTM(best_params['lstm_units_1'], activation=\"tanh\", return_sequences=True),\n",
    "    Dropout(best_params['dropout_1']),\n",
    "    LSTM(best_params['lstm_units_2'], activation=\"tanh\", return_sequences=True),\n",
    "    Dropout(best_params['dropout_2']),\n",
    "    LSTM(best_params['lstm_units_3'], activation=\"tanh\"),\n",
    "    Dropout(best_params['dropout_3']),\n",
    "    Dense(96, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "final_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss=\"mse\", metrics=['mae'])\n",
    "\n",
    "history= final_model.fit(\n",
    "    X_temp, y_temp, \n",
    "    epochs=100, batch_size=best_params['batch_size'], \n",
    "    verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history.history['loss'], label='Trainingsverlust (MSE)')\n",
    "plt.plot(history.history['mae'], label='Trainings-MAE')\n",
    "plt.xlabel(\"Epochen\")\n",
    "plt.ylabel(\"Fehler\")\n",
    "plt.title(\"Trainingsverlauf des finalen Modells\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae = final_model.evaluate(X_test, y_test)\n",
    "test_rmse = np.sqrt(test_loss)\n",
    "\n",
    "print(f\"\\n Finaler Testverlust (MSE): {test_loss:.4f}\")\n",
    "print(f\"\\n Finaler Testverlust (RMSE): {test_rmse:.4f}\")\n",
    "print(f\"Finaler Test-MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Fehlerauswertung pro Standort:\")\n",
    "y_pred = final_model.predict(X_test)\n",
    "unique_locations = np.unique(loc_test)\n",
    "\n",
    "for loc in unique_locations:\n",
    "    indices = np.where(loc_test == loc)[0]\n",
    "    y_true_loc = y_test[indices]\n",
    "    y_pred_loc = y_pred[indices]\n",
    "\n",
    "    mse = mean_squared_error(y_true_loc, y_pred_loc)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_loc, y_pred_loc)\n",
    "\n",
    "    print(f\"  Standort {loc}:\")\n",
    "    print(f\"    MSE : {mse:.4f}\")\n",
    "    print(f\"    RMSE: {rmse:.4f}\")\n",
    "    print(f\"    MAE : {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(y_test[0], label='Echte Werte')\n",
    "plt.plot(y_pred[0], label='Vorhersage')\n",
    "plt.title(\"Beispiel: Vorhersage vs. Realität (1 Testbeispiel)\")\n",
    "plt.xlabel(\"Zeitpunkte\")\n",
    "plt.ylabel(\"PV-Leistung (skaliert)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
